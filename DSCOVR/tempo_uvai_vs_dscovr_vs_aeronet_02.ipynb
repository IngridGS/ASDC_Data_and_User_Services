{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5ee391-5b83-44a5-8354-6eab9d7dde69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# TEMPO UV Aerosol Index from L2_O3TOT\n",
    "\n",
    "\n",
    "This notebook illustrates comparison of TEMPO ultra-violet aerosol index (UVAI) against DSCOVR EPIC UVAI as well as comparison of DSCOVR EPIC AOD against AERONET.\n",
    "\n",
    "Since AERONET data files cannot be downloaded programmatically, a user is supposed to upload an AERONET AOD level 1.5 or 2.0 data file and provide its name as a constant. Since TEMPO spatial coverage is regional and limited to North America, it is user's responsibilty to select AERONET station within TEMPO's field of regard (FOR). If the selected station is outside FOR, no TEMPO time series will be generated.\n",
    "\n",
    "The user is allowed to choose the time period of interest by providing start and end dates in the form YYYYMMDD. Please be aware, that if the selecte period of interest is outside of available time span of one of the sensors, corresponding time series will not be generated.\n",
    "\n",
    "TEMPO and DSCOVR data are downloaded on-the-fly with earthaccess library.\n",
    "\n",
    "TEMPO and DSCOVR L2 AER data are interpolated to the location of the selected AERONET station."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b152f-ad1b-424a-b506-34f86af18cab",
   "metadata": {},
   "source": [
    "# 1 Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aec7fe5-2fa4-4675-94c1-9c3a8fd5ddeb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import earthaccess # needed to discover and download TEMPO data\n",
    "import netCDF4 as nc # needed to read TEMPO data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import platform\n",
    "from subprocess import Popen\n",
    "import shutil\n",
    "\n",
    "from shapely.geometry import Point, Polygon # needed to search a point within a polygon\n",
    "from scipy.interpolate import griddata # needed to interpolate TEMPO data to the point of interest\n",
    "from scipy import stats # needed for linear regression analysis\n",
    "\n",
    "import requests # needed to search for and download Pandora data\n",
    "import codecs # needed to read Pandora data\n",
    "import numpy as np\n",
    "\n",
    "import h5py # needed to read DSCOVR_EPIC_L2_TO3 files\n",
    "import matplotlib.pyplot as plt # needed to plot the resulting time series\n",
    "from urllib.request import urlopen, Request # needed to search for and download Pandora data\n",
    "from pathlib import Path # needed to check whether a needed data file is already downloaded\n",
    "from datetime import datetime, timedelta # needed to work with time in plotting time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21a71b-76fe-4564-b3f5-2d4f0ca951d7",
   "metadata": {},
   "source": [
    "# 2 Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8b327-5ad1-4964-94b4-5bae511b834a",
   "metadata": {},
   "source": [
    "## 2.1 function to read DSCOVR AER data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762dcce-b0b8-4257-a533-6eaf1fee455e",
   "metadata": {},
   "source": [
    "function read_epic_l2_AER reads DSCOVR_EPIC_L2_AER product file given by its fname\n",
    "and returns arrays of 2D latitudes, longitudes, UVAI, and AOD along with their fill values, time, and wavelengths at which AOD are retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab46651-2231-47be-b758-092c280322cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_epic_l2_AER(fname):\n",
    "\n",
    "  aod_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields/FinalAerosolOpticalDepth'\n",
    "  uvai_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields/UVAerosolIndex'\n",
    "  lat_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Geolocation Fields/Latitude'\n",
    "  lon_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Geolocation Fields/Longitude'\n",
    "  wl_name = '/HDFEOS/SWATHS/Aerosol NearUV Swath/Data Fields/Wavelength'\n",
    "\n",
    "  try:\n",
    "    f = h5py.File(fname, \"r\" )\n",
    "\n",
    "    item = f[aod_name]\n",
    "    aod3D = np.array(item[:])\n",
    "    fv_aod = item.fillvalue\n",
    "\n",
    "    item = f[uvai_name]\n",
    "    uvai2D = np.array(item[:])\n",
    "    fv_uvai = item.fillvalue\n",
    "\n",
    "    item = f[lat_name]\n",
    "    lat2D = np.array(item[:])\n",
    "    fv_lat = item.fillvalue\n",
    "\n",
    "    item = f[lon_name]\n",
    "    lon2D = np.array(item[:])\n",
    "    fv_lon = item.fillvalue\n",
    "\n",
    "    item = f[wl_name]\n",
    "    wl = np.array(item[:])\n",
    "    fv_wl = item.fillvalue\n",
    "\n",
    "    f.close()\n",
    "\n",
    "# getting time from the granule's filename\n",
    "    fname_split = fname.split('_')\n",
    "    timestamp = fname_split[-2]\n",
    "    yyyy= int(timestamp[0 : 4])\n",
    "    mm = int(timestamp[4 : 6])\n",
    "    dd = int(timestamp[6 : 8])\n",
    "    hh = int(timestamp[8 : 10])\n",
    "    mn = int(timestamp[10 : 12])\n",
    "    ss = int(timestamp[12 : 14])\n",
    "\n",
    "  except:\n",
    "    print(\"Unable to find or read hdf5 input granule file \", fname)\n",
    "    aod3D  = 0.\n",
    "    fv_aod  = 0.\n",
    "    uvai2D  = 0.\n",
    "    fv_uvai  = 0.\n",
    "    lat2D  = 0.\n",
    "    fv_lat  = 0.\n",
    "    lon2D  = 0.\n",
    "    fv_lon  = 0.\n",
    "    wl  = 0.\n",
    "    fv_wl  = 0.\n",
    "    yyyy  = 0.\n",
    "    mm  = 0.\n",
    "    dd  = 0.\n",
    "    hh  = 0.\n",
    "    mn  = 0.\n",
    "    ss  = 0.\n",
    "\n",
    "  return aod3D, fv_aod, uvai2D, fv_uvai, lat2D, fv_lat, lon2D, fv_lon\\\n",
    ", wl, fv_wl, yyyy, mm, dd, hh, mn, ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea837a-122b-4a81-ad75-928800ed4862",
   "metadata": {},
   "source": [
    "## 2.2 function to read AERONET data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3c19c-91af-4338-b83b-d391b2db9f1e",
   "metadata": {},
   "source": [
    "Function read_aeronet_mw takes aeronet file name, a set of wavelengths, and start and end dates of the timeframe of interest as arguments and returns a timeseries array of AODs at these wavelengths.\n",
    "\n",
    "If the wavelength is not in the list of wavelengths of the data file,\n",
    "or if there are no retrieval at it, empty array is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adc7843e-1ee2-42cc-9f5f-2647992a9208",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_aeronet_mw(fname, wl, start_date, end_date):\n",
    "  wln = np.array(copy.deepcopy(sorted(wl)))\n",
    "  print(wln)\n",
    "  f_aero = open(fname, 'r')\n",
    "\n",
    "  while True:\n",
    "\n",
    "    line = f_aero.readline()\n",
    "    if not line: break\n",
    "    if 'Date' in line:\n",
    "      header = line.split(',')\n",
    "      nheader  = len(header)\n",
    "      break\n",
    "\n",
    "# finding positions of necessary information in the header\n",
    "# date and time are always 1st and 2nd, so no need to search for these names in the header\n",
    "  date_pos = 0\n",
    "  time_pos = 1\n",
    "# wavelengths first\n",
    "  nwl = len(wln)\n",
    "  aod_pos = np.empty([nwl], dtype = int)\n",
    "  aod_pos[:] = -1\n",
    "  for iwl in range(nwl):\n",
    "    aod_str = str('AOD_%dnm' %wln[iwl])\n",
    "    print(aod_str)\n",
    "    for i in range(nheader):\n",
    "      if header[i] == aod_str: aod_pos[iwl] = i\n",
    "\n",
    "  wln = wln[aod_pos >= 0]\n",
    "  nwl = len(wln)\n",
    "  aod_pos = aod_pos[aod_pos >= 0]\n",
    "  print(aod_pos)\n",
    "  print(wln)\n",
    "\n",
    "  if nwl == 0:\n",
    "    print('wavelengths not found')\n",
    "    aod = np.empty([0])\n",
    "    wln = []\n",
    "    date_time = np.empty([0, 2])\n",
    "    lat = -999.\n",
    "    lon = -999.\n",
    "    AERONET_name = ''\n",
    "\n",
    "    return AERONET_name, lat, lon, wln, date_time, aod\n",
    "\n",
    "# AERONET Site name\n",
    "  name_pos = -1\n",
    "  name_str = 'AERONET_Site_Name'\n",
    "  for i in range(nheader):\n",
    "    if header[i] == name_str: name_pos = i\n",
    "\n",
    "# AERONET Site latitude\n",
    "  lat_pos = -1\n",
    "  lat_str = 'Site_Latitude(Degrees)'\n",
    "  for i in range(nheader):\n",
    "    if header[i] == lat_str: lat_pos = i\n",
    "\n",
    "# AERONET Site longitude\n",
    "  lon_pos = -1\n",
    "  lon_str = 'Site_Longitude(Degrees)'\n",
    "  for i in range(nheader):\n",
    "    if header[i] == lon_str: lon_pos = i\n",
    "\n",
    "  aod = np.empty([0, nwl])\n",
    "  date_time = np.empty([0, 2])\n",
    "# reading the 1st line of data, get name, lat, lon,\n",
    "# and if aod is valid append arrays aod and date_time\n",
    "  line = f_aero.readline()\n",
    "  values = line.split(',')\n",
    "  AERONET_name = values[name_pos]\n",
    "  lat = float(values[lat_pos])\n",
    "  lon = float(values[lon_pos])\n",
    "  aod_loc = np.empty([nwl])\n",
    "  date_loc = values[date_pos]\n",
    "  time_loc = values[time_pos]\n",
    "\n",
    "  dd = int(date_loc[0:2])\n",
    "  mm = int(date_loc[3:5])\n",
    "  yyyy = int(date_loc[6:10])\n",
    "  date_stamp = yyyy*10000 + mm*100 + dd\n",
    "  if date_stamp >= start_date and date_stamp <= end_date:\n",
    "    for iwl in range(nwl): aod_loc[iwl] = float(values[aod_pos[iwl]])\n",
    "    aod = np.append(aod, [aod_loc], axis = 0)\n",
    "    date_time = np.append(date_time, [[date_loc, time_loc]], axis = 0)\n",
    "\n",
    "# reading all other lines of data,\n",
    "# if aod is valid append arrays aod and date_time\n",
    "\n",
    "  while True:\n",
    "    line = f_aero.readline()\n",
    "    if not line: break\n",
    "    values = line.split(',')\n",
    "\n",
    "    aod_loc = np.empty([nwl])\n",
    "    date_loc = values[date_pos]\n",
    "    time_loc = values[time_pos]\n",
    "\n",
    "    dd = int(date_loc[0:2])\n",
    "    mm = int(date_loc[3:5])\n",
    "    yyyy = int(date_loc[6:10])\n",
    "    date_stamp = yyyy*10000 + mm*100 + dd\n",
    "    if date_stamp < start_date or date_stamp > end_date: continue\n",
    "\n",
    "    for iwl in range(nwl): aod_loc[iwl] = float(values[aod_pos[iwl]])\n",
    "\n",
    "    aod = np.append(aod, [aod_loc], axis = 0)\n",
    "    date_time = np.append(date_time, [[date_loc, time_loc]], axis = 0)\n",
    "\n",
    "  f_aero.close()\n",
    "\n",
    "  return AERONET_name, lat, lon, wln, date_time, aod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc5689-b1c1-44ee-94e4-c50fe58039c9",
   "metadata": {},
   "source": [
    "# 2.3 functions to work with TEMPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64f2ed-e7ea-4f83-84d6-7b15b6a35542",
   "metadata": {},
   "source": [
    "### 2.3.1 function to read UVAI from TEMPO_O3TOT_L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd199a-7921-4c8c-ad15-d8aeca6c564b",
   "metadata": {},
   "source": [
    "function read_TEMPO_O3TOT_L2_UVAI reads the following arrays from the\n",
    "TEMPO L2 O3TOT product TEMPO_O3TOT_L2_V01(2):\n",
    "\n",
    "  uv_aerosol_index;\n",
    "\n",
    "  quality_flag;\n",
    "\n",
    "and returns respective fields along with their fill values and coordinates of the pixels.\n",
    "\n",
    "If one requested variables cannot be read, all returned variables are zeroed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ad4f0c-d716-47da-93ef-6e78089340ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_TEMPO_O3TOT_L2_UVAI(fn):\n",
    "  var_name = 'uv_aerosol_index'\n",
    "  var_QF_name = 'quality_flag'\n",
    "\n",
    "  try:\n",
    "    ds = nc.Dataset(fn)\n",
    "\n",
    "    prod = ds.groups['product'] # this opens group product, /product, as prod\n",
    "\n",
    "    var = prod.variables[var_name] # this reads variable column_amount_o3 from prod (group product, /product)\n",
    "    uvai = np.array(var)\n",
    "    uvai_fv = var.getncattr('_FillValue')\n",
    "\n",
    "    var_QF = prod.variables[var_QF_name] # this reads variable column_amount_o3 from prod (group product, /product)\n",
    "    uvai_QF = np.array(var_QF)\n",
    "# there is no fill value for the quality flag.\n",
    "# Once it is available in the next version of the product,\n",
    "# un-comment the line below and add fv_QF to the return line.\n",
    "#    fv_QF = var_QF.getncattr('_FillValue')\n",
    "\n",
    "    geo = ds.groups['geolocation'] # this opens group geolocation, /geolocation, as geo\n",
    "\n",
    "    lat = np.array(geo.variables['latitude']) # this reads variable latitude from geo (geolocation group, /geolocation) into a numpy array\n",
    "    lon = np.array(geo.variables['longitude']) # this reads variable longitude from geo (geolocation group, /geolocation) into a numpy array\n",
    "    fv_geo = geo.variables['latitude'].getncattr('_FillValue')\n",
    "# it appeared that garbage values of latitudes and longitudes in the L2 files\n",
    "# are 9.969209968386869E36 while fill value is -1.2676506E30\n",
    "# (after deeper search it was found that actual value in the file is -1.2676506002282294E30).\n",
    "# For this reason, fv_geo is set to 9.96921E36 to make the code working.\n",
    "# Once the problem is resolved and garbage values of latitudes and longitudes\n",
    "# equal to their fill value, the line below must be removed.\n",
    "    fv_geo = 9.969209968386869E36\n",
    "\n",
    "    time = np.array(geo.variables['time'] )# this reads variable longitude from geo (geolocation group, /geolocation) into a numpy array\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "  except:\n",
    "    print('variable '+var_name+' cannot be read in file '+fn)\n",
    "    lat = 0.\n",
    "    lon = 0.\n",
    "    time = 0.\n",
    "    fv_geo = 0.\n",
    "    uvai = 0.\n",
    "    uvai_QF = 0.\n",
    "    fv_uvai = 0.\n",
    "#    fv_QF = -999\n",
    "    prod_unit = ''\n",
    "\n",
    "  return lat, lon, fv_geo, time, uvai, uvai_QF, uvai_fv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e00996-a707-49db-8c3b-342f643571ba",
   "metadata": {},
   "source": [
    "### 2.3.2 function creating TEMPO O3 granule polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963a5ae7-c4bb-46c2-b53c-2147ea690b9a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def TEMPO_L2_polygon(lat, lon, fv_geo):\n",
    "  nx = lon.shape[0]\n",
    "  ny = lon.shape[1]\n",
    "  print('granule has %3d scanlines by %4d pixels' %(nx, ny))\n",
    "\n",
    "  dpos = np.empty([0,2])\n",
    "\n",
    "  x_ind = np.empty([nx, ny], dtype = int) # creating array in x indices\n",
    "  for ix in range(nx): x_ind[ix, :] = ix # populating array in x indices\n",
    "  y_ind = np.empty([nx, ny], dtype = int)\n",
    "  for iy in range(ny): y_ind[:, iy] = iy # populating array in x indices\n",
    "\n",
    "  mask = (lon[ix, iy] != fv_geo)&(lat[ix, iy] != fv_geo)\n",
    "  if len(lon[mask]) == 0:\n",
    "    print('the granule is empty - no meaningful positions')\n",
    "    return dpos\n",
    "\n",
    "# right boundary\n",
    "  r_m = min(x_ind[mask].flatten())\n",
    "  local_mask = (lon[r_m, :] != fv_geo)&(lat[r_m, :] != fv_geo)\n",
    "  r_b = np.stack((lon[r_m, local_mask], lat[r_m, local_mask])).T\n",
    "\n",
    "# left boundary\n",
    "  l_m = max(x_ind[mask].flatten())\n",
    "  local_mask = (lon[l_m, :] != fv_geo)&(lat[l_m, :] != fv_geo)\n",
    "  l_b = np.stack((lon[l_m, local_mask], lat[l_m, local_mask])).T\n",
    "\n",
    "#top and bottom boundaries\n",
    "  t_b = np.empty([0,2])\n",
    "  b_b = np.empty([0,2])\n",
    "  for ix in range(r_m + 1, l_m):\n",
    "    local_mask = (lon[ix, :] != fv_geo)&(lat[ix, :] != fv_geo)\n",
    "    local_y_ind = y_ind[ix, local_mask]\n",
    "    y_ind_top = min(local_y_ind)\n",
    "    y_ind_bottom = max(local_y_ind)\n",
    "    t_b = np.append(t_b, [[lon[ix, y_ind_top], lat[ix, y_ind_top]]], axis=0)\n",
    "    b_b = np.append(b_b, [[lon[ix, y_ind_bottom], lat[ix, y_ind_bottom]]], axis=0)\n",
    "\n",
    "# combining right, top, left, and bottom boundaries together, going along the combined boundary counterclockwise\n",
    "  dpos = np.append(dpos, r_b[ : :-1, :], axis=0) # this adds right boundary, counterclockwise\n",
    "  dpos = np.append(dpos, t_b, axis=0) # this adds top boundary, counterclockwise\n",
    "  dpos = np.append(dpos, l_b, axis=0) # this adds left boundary, counterclockwise\n",
    "  dpos = np.append(dpos, b_b[ : :-1, :], axis=0) # this adds bottom boundary, counterclockwise\n",
    "\n",
    "  print('polygon shape: ',dpos.shape)\n",
    "\n",
    "  return dpos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2f56d-d2f2-47e0-815f-bab4d39aee66",
   "metadata": {},
   "source": [
    "# Main code begins here\n",
    "# 3 Establishing access to EarthData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d709a1-ca05-4a47-94ac-78e88e41aadd",
   "metadata": {},
   "source": [
    "User needs to create an account at https://www.earthdata.nasa.gov/\n",
    "Function earthaccess.login prompts for EarthData login and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65fe3fac-cb6e-4809-87b5-0d38b68b2c90",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved .dodsrc to: /home/jovyan/\n"
     ]
    }
   ],
   "source": [
    "# logging in\n",
    "auth = earthaccess.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "# Creating local directory\n",
    "homeDir = os.path.expanduser(\"~\") + os.sep\n",
    "\n",
    "with open(homeDir + '.dodsrc', 'w') as file:\n",
    "    file.write('HTTP.COOKIEJAR={}.urs_cookies\\n'.format(homeDir))\n",
    "    file.write('HTTP.NETRC={}.netrc'.format(homeDir))\n",
    "    file.close()\n",
    "\n",
    "print('Saved .dodsrc to:', homeDir)\n",
    "\n",
    "# Set appropriate permissions for Linux/macOS\n",
    "if platform.system() != \"Windows\":\n",
    "    Popen('chmod og-rw ~/.netrc', shell=True)\n",
    "else:\n",
    "    # Copy dodsrc to working directory in Windows\n",
    "    shutil.copy2(homeDir + '.dodsrc', os.getcwd())\n",
    "    print('Copied .dodsrc to:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f8900-121d-4c23-ac35-668462b6e002",
   "metadata": {},
   "source": [
    "# 4 Select timeframe of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc1858-342f-4a63-bc50-cd0d7903aee9",
   "metadata": {},
   "source": [
    "the timeframe is going to be common for all instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d6eac4a-16c1-4989-ae1c-f533244c8358",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter period of interest, start and end dates, in the form YYYYMMDD\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter start date of interest  20230802\n",
      "enter end date of interest  20230802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 8 2 2023 8 2\n"
     ]
    }
   ],
   "source": [
    "print('enter period of interest, start and end dates, in the form YYYYMMDD')\n",
    "datestamp_ini = input('enter start date of interest ')\n",
    "datestamp_fin = input('enter end date of interest ')\n",
    "\n",
    "start_date = int(datestamp_ini)\n",
    "end_date = int(datestamp_fin)\n",
    "\n",
    "yyyy_ini = start_date//10000\n",
    "mm_ini = (start_date//100 - yyyy_ini*100)\n",
    "dd_ini = (start_date - yyyy_ini*10000 - mm_ini*100)\n",
    "\n",
    "yyyy_fin = end_date//10000\n",
    "mm_fin = (end_date//100 - yyyy_fin*100)\n",
    "dd_fin = (end_date - yyyy_fin*10000 - mm_fin*100)\n",
    "print(yyyy_ini, mm_ini, dd_ini, yyyy_fin, mm_fin, dd_fin)\n",
    "\n",
    "date_start = str('%4.4i-%2.2i-%2.2i 00:00:00' %(yyyy_ini, mm_ini, dd_ini))\n",
    "date_end = str('%4.4i-%2.2i-%2.2i 23:59:59' %(yyyy_fin, mm_fin, dd_fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06cba8-7e84-4755-9360-439cf5a71a78",
   "metadata": {},
   "source": [
    "## 4.1 calculate datetime objects for the beginning and end of the timeframe of interest\n",
    "using datetime library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df13e8a3-e590-49d3-812d-922ee3e11605",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "yyyy = yyyy_ini\n",
    "mm = mm_ini\n",
    "dd = dd_ini\n",
    "hh = 0\n",
    "mn = 0\n",
    "ss = 0\n",
    "us = 0 # microseconds\n",
    "dt_ini = datetime(yyyy, mm, dd, hh, mn, ss, us)\n",
    "\n",
    "yyyy = yyyy_fin\n",
    "mm = mm_fin\n",
    "dd = dd_fin\n",
    "hh = 23\n",
    "mn = 59\n",
    "ss = 59\n",
    "us = 999999 # microseconds\n",
    "dt_fin = datetime(yyyy, mm, dd, hh, mn, ss, us) # this is time 1 second before the end of the timeframe of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfdfea4-a8dc-49b8-833c-e47fe511d511",
   "metadata": {},
   "source": [
    "# 5 work with AERONET data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83ce8b-63f7-4432-bc22-81545b04140b",
   "metadata": {},
   "source": [
    "As of now, April 24, 2024, AERONET data cannot be downloaded programmatically.\n",
    "User must upload an AERONET data file using AERONET download tool here https://aeronet.gsfc.nasa.gov/new_web/webtool_aod_v3.html.\n",
    "The first line below is the name of the AERONET file used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a2bc6b0-be74-46c3-bfce-c455991fa51f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fn = '20230801_20241231_CCNY.lev15'\n",
    "wl = [500, 340, 380]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d90a6b-4627-4610-a5a1-e81fcb3d1b55",
   "metadata": {},
   "source": [
    "## 5.1 read AERONET data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d74f85f-f81f-4dfb-b42d-c9e6f1e97c05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[340 380 500]\n",
      "AOD_340nm\n",
      "AOD_380nm\n",
      "AOD_500nm\n",
      "[25 24 18]\n",
      "[340 380 500]\n",
      "[340 380 500]\n",
      "71\n",
      "name CCNY, latitude   40.8213, longitude   -73.9490\n",
      "340 69\n",
      "380 69\n",
      "500 71\n"
     ]
    }
   ],
   "source": [
    "AERONET_name, lat, lon, wln, date_time, aod = read_aeronet_mw(fn, wl, start_date, end_date)\n",
    "POI_lat = lat\n",
    "POI_lon = lon\n",
    "POI_name = AERONET_name\n",
    "nt = len(date_time)\n",
    "\n",
    "print(wln)\n",
    "print(len(aod))\n",
    "print('name %s, latitude %9.4f, longitude %10.4f' %(AERONET_name, lat, lon))\n",
    "\n",
    "nwl = len(wln)\n",
    "for iwl in range(nwl): print(wln[iwl], len(aod[aod[:,iwl]>0, iwl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52e6899-c127-433c-b8e1-299533c96ffb",
   "metadata": {},
   "source": [
    "## 5.2 create timeseries of AERONET AODs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea12eab-6d74-4155-b7f4-fa54ce91ca42",
   "metadata": {},
   "source": [
    "calculate time difference, dy_loc, in fractional days between AERONET observations\n",
    "and the beginning of the timeframe of interest,\n",
    "write date and time as yyyy, mm, dd, hh, mn, ss, dt_loc\n",
    "along with wavelengths and corresponding AODs to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0573d3ce-db66-4994-8002-28204dfdc132",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['02:08:2023' '11:52:26'] 2023 8 2 11 52 26 0.49474537037037036 [0.167633 0.150369 0.104141]\n",
      "['02:08:2023' '11:57:39'] 2023 8 2 11 57 39 0.4983680555555556 [0.177528 0.159516 0.109894]\n",
      "['02:08:2023' '12:01:26'] 2023 8 2 12 1 26 0.5009953703703703 [0.181789 0.163283 0.112015]\n",
      "['02:08:2023' '12:07:39'] 2023 8 2 12 7 39 0.5053125 [0.183797 0.165369 0.114096]\n",
      "['02:08:2023' '12:12:03'] 2023 8 2 12 12 3 0.5083680555555555 [0.182201 0.164005 0.113111]\n",
      "['02:08:2023' '12:17:39'] 2023 8 2 12 17 39 0.5122569444444445 [0.176299 0.158314 0.108334]\n",
      "['02:08:2023' '12:22:39'] 2023 8 2 12 22 39 0.5157291666666667 [0.179609 0.161741 0.110283]\n",
      "['02:08:2023' '12:24:49'] 2023 8 2 12 24 49 0.5172337962962963 [0.185426 0.166782 0.114021]\n",
      "['02:08:2023' '12:27:39'] 2023 8 2 12 27 39 0.5192013888888889 [0.191355 0.172065 0.117113]\n",
      "['02:08:2023' '12:32:39'] 2023 8 2 12 32 39 0.5226736111111111 [0.190814 0.171744 0.11646 ]\n",
      "['02:08:2023' '12:37:39'] 2023 8 2 12 37 39 0.5261458333333333 [0.195309 0.176786 0.122912]\n",
      "['02:08:2023' '12:49:44'] 2023 8 2 12 49 44 0.534537037037037 [0.205968 0.185525 0.124779]\n",
      "['02:08:2023' '13:04:53'] 2023 8 2 13 4 53 0.5450578703703703 [0.237061 0.214833 0.144355]\n",
      "['02:08:2023' '13:14:05'] 2023 8 2 13 14 5 0.5514467592592592 [0.258304 0.233996 0.15743 ]\n",
      "['02:08:2023' '13:29:50'] 2023 8 2 13 29 50 0.5623842592592593 [0.196156 0.177017 0.119275]\n",
      "['02:08:2023' '13:39:08'] 2023 8 2 13 39 8 0.5688425925925926 [0.185158 0.168224 0.112152]\n",
      "['02:08:2023' '13:54:21'] 2023 8 2 13 54 21 0.5794097222222222 [0.195882 0.179183 0.118869]\n",
      "['02:08:2023' '14:03:42'] 2023 8 2 14 3 42 0.5859027777777778 [0.18666  0.168886 0.112149]\n",
      "['02:08:2023' '14:19:30'] 2023 8 2 14 19 30 0.596875 [0.205253 0.186197 0.123388]\n",
      "['02:08:2023' '14:21:31'] 2023 8 2 14 21 31 0.598275462962963 [0.199085 0.179287 0.118839]\n",
      "['02:08:2023' '14:23:33'] 2023 8 2 14 23 33 0.5996875 [0.202622 0.183128 0.120295]\n",
      "['02:08:2023' '14:25:35'] 2023 8 2 14 25 35 0.601099537037037 [0.208272 0.187998 0.124521]\n",
      "['02:08:2023' '14:57:40'] 2023 8 2 14 57 40 0.6233796296296297 [-9.99000e+02 -9.99000e+02  1.75868e-01]\n",
      "['02:08:2023' '15:27:57'] 2023 8 2 15 27 57 0.6444097222222223 [0.450454 0.404168 0.264271]\n",
      "['02:08:2023' '15:32:40'] 2023 8 2 15 32 40 0.6476851851851851 [0.484111 0.434443 0.285494]\n",
      "['02:08:2023' '15:37:40'] 2023 8 2 15 37 40 0.6511574074074075 [0.452174 0.40588  0.266029]\n",
      "['02:08:2023' '15:42:40'] 2023 8 2 15 42 40 0.6546296296296297 [0.496932 0.44459  0.292271]\n",
      "['02:08:2023' '15:57:40'] 2023 8 2 15 57 40 0.6650462962962963 [0.524143 0.470849 0.309345]\n",
      "['02:08:2023' '16:02:40'] 2023 8 2 16 2 40 0.6685185185185185 [0.494714 0.443077 0.290863]\n",
      "['02:08:2023' '16:14:30'] 2023 8 2 16 14 30 0.6767361111111111 [0.500429 0.447167 0.291115]\n",
      "['02:08:2023' '16:30:17'] 2023 8 2 16 30 17 0.6876967592592592 [0.452087 0.405042 0.26571 ]\n",
      "['02:08:2023' '16:32:41'] 2023 8 2 16 32 41 0.6893634259259259 [0.474824 0.421059 0.26579 ]\n",
      "['02:08:2023' '17:30:50'] 2023 8 2 17 30 50 0.7297453703703703 [0.169886 0.151935 0.094205]\n",
      "['02:08:2023' '17:42:41'] 2023 8 2 17 42 41 0.7379745370370371 [-9.99000e+02 -9.99000e+02  1.29848e-01]\n",
      "['02:08:2023' '17:47:40'] 2023 8 2 17 47 40 0.7414351851851851 [0.208386 0.185737 0.120282]\n",
      "['02:08:2023' '17:52:41'] 2023 8 2 17 52 41 0.7449189814814815 [0.216437 0.195054 0.125919]\n",
      "['02:08:2023' '18:02:40'] 2023 8 2 18 2 40 0.7518518518518519 [0.205661 0.185377 0.121323]\n",
      "['02:08:2023' '18:05:00'] 2023 8 2 18 5 0 0.7534722222222222 [0.199723 0.179063 0.116503]\n",
      "['02:08:2023' '18:31:14'] 2023 8 2 18 31 14 0.7716898148148148 [0.189355 0.167719 0.106521]\n",
      "['02:08:2023' '18:37:40'] 2023 8 2 18 37 40 0.7761574074074075 [0.18439  0.162167 0.103897]\n",
      "['02:08:2023' '18:42:40'] 2023 8 2 18 42 40 0.7796296296296297 [0.18334  0.163672 0.106053]\n",
      "['02:08:2023' '18:47:40'] 2023 8 2 18 47 40 0.7831018518518519 [0.208059 0.184654 0.120211]\n",
      "['02:08:2023' '18:52:40'] 2023 8 2 18 52 40 0.7865740740740741 [0.195461 0.174641 0.11203 ]\n",
      "['02:08:2023' '18:57:40'] 2023 8 2 18 57 40 0.7900462962962963 [0.214311 0.192524 0.127887]\n",
      "['02:08:2023' '19:12:51'] 2023 8 2 19 12 51 0.8005902777777778 [0.262352 0.23469  0.155023]\n",
      "['02:08:2023' '19:28:18'] 2023 8 2 19 28 18 0.8113194444444445 [0.264377 0.237628 0.160266]\n",
      "['02:08:2023' '19:32:40'] 2023 8 2 19 32 40 0.8143518518518519 [0.253449 0.230219 0.156437]\n",
      "['02:08:2023' '19:42:40'] 2023 8 2 19 42 40 0.8212962962962963 [0.292735 0.264179 0.172788]\n",
      "['02:08:2023' '19:57:40'] 2023 8 2 19 57 40 0.8317129629629629 [0.300768 0.283408 0.187471]\n",
      "['02:08:2023' '20:02:40'] 2023 8 2 20 2 40 0.8351851851851851 [0.275669 0.251891 0.175997]\n",
      "['02:08:2023' '20:11:53'] 2023 8 2 20 11 53 0.8415856481481482 [0.260447 0.237608 0.166235]\n",
      "['02:08:2023' '20:26:58'] 2023 8 2 20 26 58 0.8520601851851852 [0.231016 0.211302 0.147655]\n",
      "['02:08:2023' '20:36:09'] 2023 8 2 20 36 9 0.8584375 [0.2405   0.217921 0.149091]\n",
      "['02:08:2023' '20:51:50'] 2023 8 2 20 51 50 0.8693287037037037 [0.243688 0.223337 0.157807]\n",
      "['02:08:2023' '20:53:50'] 2023 8 2 20 53 50 0.8707175925925926 [0.23527  0.215746 0.153473]\n",
      "['02:08:2023' '20:55:49'] 2023 8 2 20 55 49 0.8720949074074074 [0.232668 0.21348  0.151181]\n",
      "['02:08:2023' '20:57:48'] 2023 8 2 20 57 48 0.8734722222222222 [0.236663 0.217494 0.154321]\n",
      "['02:08:2023' '21:06:59'] 2023 8 2 21 6 59 0.879849537037037 [0.234972 0.215889 0.154752]\n",
      "['02:08:2023' '21:22:08'] 2023 8 2 21 22 8 0.8903703703703704 [0.237397 0.218635 0.157462]\n",
      "['02:08:2023' '21:31:20'] 2023 8 2 21 31 20 0.8967592592592593 [0.279013 0.259029 0.190824]\n",
      "['02:08:2023' '21:47:05'] 2023 8 2 21 47 5 0.9076967592592593 [0.259149 0.241348 0.179497]\n",
      "['02:08:2023' '21:56:18'] 2023 8 2 21 56 18 0.9140972222222222 [0.258039 0.238991 0.176168]\n",
      "['02:08:2023' '22:11:30'] 2023 8 2 22 11 30 0.9246527777777778 [0.24559  0.224214 0.161081]\n",
      "['02:08:2023' '22:20:40'] 2023 8 2 22 20 40 0.9310185185185185 [0.256123 0.233352 0.167165]\n",
      "['02:08:2023' '22:36:24'] 2023 8 2 22 36 24 0.9419444444444445 [0.356607 0.334492 0.254117]\n",
      "['02:08:2023' '22:42:19'] 2023 8 2 22 42 19 0.9460532407407407 [0.319526 0.299427 0.227325]\n",
      "['02:08:2023' '22:44:16'] 2023 8 2 22 44 16 0.9474074074074074 [0.308779 0.288391 0.216689]\n",
      "['02:08:2023' '22:45:59'] 2023 8 2 22 45 59 0.948599537037037 [0.31427  0.29255  0.219464]\n",
      "['02:08:2023' '22:47:42'] 2023 8 2 22 47 42 0.9497916666666667 [0.319625 0.298565 0.224658]\n",
      "['02:08:2023' '22:56:53'] 2023 8 2 22 56 53 0.9561689814814814 [0.32606  0.304828 0.229832]\n",
      "['02:08:2023' '23:12:35'] 2023 8 2 23 12 35 0.9670717592592593 [0.305476 0.279301 0.204674]\n"
     ]
    }
   ],
   "source": [
    "out_Q_AERONET = 'AOD_AERONET'\n",
    "fout = open(out_Q_AERONET+'_'+datestamp_ini+'_'+datestamp_fin+'_'\\\n",
    "+POI_name+'_'+str('%08.4fN_%08.4fW.txt' %(POI_lat, -POI_lon)), 'w')\n",
    "fout.write('timeseries of '+out_Q_AERONET+' at '+POI_name+' '+str('%08.4fN %08.4fW' %(POI_lat, -POI_lon))+'\\n')\n",
    "fout.write('yyyy mm dd hh mn ss time,days wl,nm    AOD wl,nm    AOD wl,nm    AOD\\n')\n",
    "\n",
    "for it in range(nt):\n",
    "  d_t = date_time[it]\n",
    "  yyyy = int(d_t[0][6:10]) \n",
    "  mm = int(d_t[0][3:5])\n",
    "  dd = int(d_t[0][0:2])\n",
    "  hh = int(d_t[1][0:2])\n",
    "  mn = int(d_t[1][3:5])\n",
    "  ss = int(d_t[1][6:8])\n",
    "  us = 0 # microseconds\n",
    "  dt_AERONET = datetime(yyyy, mm, dd, hh, mn, ss, us)\n",
    "  dt_loc = (dt_AERONET - dt_ini).total_seconds()/86400\n",
    "  fout.write('%4i %2.2i %2.2i %2.2i %2.2i %2.2i %9.6f' %(yyyy, mm, dd, hh, mn, ss, dt_loc))\n",
    "  for wl, tau in zip(wln, aod[it]): fout.write(' %5.0f %6.3f' %(wl, max(tau, -1.)))\n",
    "  fout.write('\\n')\n",
    "  print(d_t, yyyy, mm, dd, hh, mn, ss, dt_loc, aod[it])\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd6da3-c3e9-4cc0-9cd8-0f41b96a50fd",
   "metadata": {},
   "source": [
    "# 6 work with DSCOVR data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b6851-228d-48d3-bfc2-73e3f161bd2f",
   "metadata": {},
   "source": [
    "## 6.1 search for DSCOVR EPIC granules\n",
    "falling into the timeframe of interest and 1 degree box surrounding the AERONET location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7fb0d70-3c6a-4111-944f-eef1268071a4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 11\n",
      "total number of DSCOVR EPIC L2_AER granules found for POI CCNY \n",
      "within period of interes between 2023-08-02 00:00:00 and 2023-08-02 23:59:59 \n",
      "is 11\n"
     ]
    }
   ],
   "source": [
    "short_name = 'DSCOVR_EPIC_L2_AER' # collection name to search for in the EarthData\n",
    "\n",
    "bbox = (POI_lon - 0.5, POI_lat - 0.5, POI_lon + 0.5, POI_lat + 0.5)\n",
    "\n",
    "POI_results_EPIC = earthaccess.search_data(short_name = short_name\\\n",
    "                                         , temporal = (date_start, date_end)\\\n",
    "                                         , bounding_box = bbox)\n",
    "\n",
    "n_EPIC = len(POI_results_EPIC)\n",
    "\n",
    "print('total number of DSCOVR EPIC L2_AER granules found for POI', POI_name\\\n",
    "    , '\\nwithin period of interes between', date_start, 'and', date_end, '\\nis', n_EPIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81346877-0f4c-40a8-9260-98988d48ecfb",
   "metadata": {},
   "source": [
    "## 6.2 ensure all granules have download links\n",
    "lines below before the call of earthaccess.download()\n",
    "check whether all found granules have download links.\n",
    "granules without links are removed from the list of search results\n",
    "without this step, those granules crash the call of earthaccess.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0f28c8e-0ab0-424b-87c1-eec2a1d8c8c1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802185036_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802210131_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802111225_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802121752_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802195603_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802142847_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802174509_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802153414_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802132319_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802163941_03.he5\n",
      "https://asdc.larc.nasa.gov/data/DSCOVR/EPIC/L2_AER_03/2023/08/DSCOVR_EPIC_L2_AER_03_20230802220658_03.he5\n"
     ]
    }
   ],
   "source": [
    "granule_links_EPIC = []\n",
    "POI_results_EPIC_bad = []\n",
    "for result in POI_results_EPIC:\n",
    "  try:\n",
    "    granule_links_EPIC.append(result['umm']['RelatedUrls'][0]['URL'])\n",
    "  except:\n",
    "    POI_results_EPIC_bad.append(result)\n",
    "\n",
    "for granule_link in granule_links_EPIC: print(granule_link)\n",
    "for result in POI_results_EPIC_bad: POI_results_EPIC.remove(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb861bd5-d586-4a00-8bd3-9c7ebb6b9493",
   "metadata": {},
   "source": [
    "## 6.3 download DSCOVR EPIC granules using earthaccess library\n",
    "check whether all downloads were successful and try another time to get the failed granules\n",
    "if the second attempt was not successful, remove failed granules from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ef5b49b-8c05-4fe2-b529-31c46411f937",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Getting 11 granules, approx download size: 0.0 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94cc0d517494c8caefc48c66b139994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0ea54f068a45e39a56ef9307f947d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949d2ba54b20429785a7422ff336ce9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downloaded_files = earthaccess.download(POI_results_EPIC, local_path='.',)\n",
    "\n",
    "# Checking whether all DSCOVR EPIC data files have been downloaded\n",
    "for granule_link in granule_links_EPIC:\n",
    "  EPIC_fname = granule_link.split('/')[-1]\n",
    "# check if file exists in the local directory\n",
    "  if not os.path.exists(EPIC_fname):\n",
    "    print(EPIC_fname, 'does not exist in local directory')\n",
    "# repeat attempt to download\n",
    "    downloaded_files = earthaccess.download(granule_link,\n",
    "                                            local_path='.')\n",
    "# if file still does not exist in the directory, remove its link from the list of links\n",
    "    if not os.path.exists(EPIC_fname): granule_links_EPIC.remove(granule_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae9fbe-56d2-4c52-8fc6-e7645d89797b",
   "metadata": {},
   "source": [
    "## 6.4 read DSCOVR EPIC granules and compile timeseries of AODs and UVAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6599d37d-3020-49dc-87f9-ac7723fd5094",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSCOVR_EPIC_L2_AER_03_20230802111225_03.he5\n",
      "2023 8 2 11 12 25 0.46695601851851853\n",
      "340.0 -0.999\n",
      "388.0 -0.999\n",
      "500.0 -0.999\n",
      "UVAI -99.0\n",
      "DSCOVR_EPIC_L2_AER_03_20230802121752_03.he5\n",
      "2023 8 2 12 17 52 0.5124074074074074\n",
      "340.0 -0.999\n",
      "388.0 -0.999\n",
      "500.0 -0.999\n",
      "UVAI -99.0\n",
      "DSCOVR_EPIC_L2_AER_03_20230802132319_03.he5\n",
      "2023 8 2 13 23 19 0.5578587962962963\n",
      "340.0 0.4353439211845398\n",
      "388.0 0.40144097805023193\n",
      "500.0 0.34441304206848145\n",
      "UVAI 1.709758460521698\n",
      "DSCOVR_EPIC_L2_AER_03_20230802142847_03.he5\n",
      "2023 8 2 14 28 47 0.6033217592592592\n",
      "340.0 0.427091121673584\n",
      "388.0 0.3938308656215668\n",
      "500.0 0.33788400888442993\n",
      "UVAI 1.2530654668807983\n",
      "DSCOVR_EPIC_L2_AER_03_20230802153414_03.he5\n",
      "2023 8 2 15 34 14 0.6487731481481481\n",
      "340.0 1.382229506969452\n",
      "388.0 1.27458655834198\n",
      "500.0 1.0935212969779968\n",
      "UVAI 1.943002998828888\n",
      "DSCOVR_EPIC_L2_AER_03_20230802163941_03.he5\n",
      "2023 8 2 16 39 41 0.694224537037037\n",
      "340.0 1.4222893118858337\n",
      "388.0 1.3115266561508179\n",
      "500.0 1.1252137422561646\n",
      "UVAI 1.3676859140396118\n",
      "DSCOVR_EPIC_L2_AER_03_20230802174509_03.he5\n",
      "2023 8 2 17 45 9 0.7396875\n",
      "340.0 1.4766647815704346\n",
      "388.0 1.3616676330566406\n",
      "500.0 1.1682318449020386\n",
      "UVAI 1.543965220451355\n",
      "DSCOVR_EPIC_L2_AER_03_20230802185036_03.he5\n",
      "2023 8 2 18 50 36 0.7851388888888889\n",
      "340.0 0.9639931174975775\n",
      "388.0 0.8453134906963775\n",
      "500.0 0.6521161487893861\n",
      "UVAI 0.7843428582769572\n",
      "DSCOVR_EPIC_L2_AER_03_20230802195603_03.he5\n",
      "2023 8 2 19 56 3 0.8305902777777778\n",
      "340.0 1.192875325679779\n",
      "388.0 1.0999786257743835\n",
      "500.0 0.9437177777290344\n",
      "UVAI 1.6863856315612793\n",
      "DSCOVR_EPIC_L2_AER_03_20230802210131_03.he5\n",
      "2023 8 2 21 1 31 0.8760532407407408\n",
      "340.0 0.7190786004066467\n",
      "388.0 0.6630794405937195\n",
      "500.0 0.5688836574554443\n",
      "UVAI 1.8348299264907837\n",
      "DSCOVR_EPIC_L2_AER_03_20230802220658_03.he5\n",
      "2023 8 2 22 6 58 0.9215046296296296\n",
      "340.0 0.8241634368896484\n",
      "388.0 0.7599806487560272\n",
      "500.0 0.6520192623138428\n",
      "UVAI 1.8943052887916565\n"
     ]
    }
   ],
   "source": [
    "# parameter geo_deviation defines maximum difference between lats and lons of EPIC pixels and that of AERONET\n",
    "geo_deviation = 0.075\n",
    "pp = np.array([POI_lon, POI_lat])\n",
    "\n",
    "out_Q_EPIC = 'AOD_UVAI_EPIC'\n",
    "\n",
    "fout = open(out_Q_EPIC+'_'+datestamp_ini+'_'+datestamp_fin+'_'\\\n",
    "+POI_name+'_'+str('%08.4fN_%08.4fW.txt' %(POI_lat, -POI_lon)), 'w')\n",
    "fout.write('timeseries of '+out_Q_EPIC+' at '+POI_name+' '+str('%08.4fN %08.4fW' %(POI_lat, -POI_lon))+'\\n')\n",
    "fout.write('yyyy mm dd hh mn ss time,days wl,nm    AOD wl,nm    AOD wl,nm    AOD UVAI\\n')\n",
    "\n",
    "for granule_link in sorted(granule_links_EPIC):\n",
    "  EPIC_fname = granule_link.split('/')[-1]\n",
    "  print(EPIC_fname)\n",
    "\n",
    "  aod3D, fv_aod, uvai2D, fv_uvai, lat2D, fv_lat, lon2D, fv_lon\\\n",
    ", wl, fv_wl, yyyy, mm, dd, hh, mn, ss = read_epic_l2_AER(EPIC_fname)\n",
    "\n",
    "  dt_EPIC = datetime(yyyy, mm, dd, hh, mn, ss)\n",
    "  dt_loc = (dt_EPIC - dt_ini).total_seconds()/86400\n",
    "  print(yyyy, mm, dd, hh, mn, ss, (dt_EPIC - dt_ini).total_seconds()/86400)\n",
    "  fout.write('%4i %2.2i %2.2i %2.2i %2.2i %2.2i %9.6f' %(yyyy, mm, dd, hh, mn, ss, dt_loc))\n",
    "\n",
    "# check whether POI is in the granule. If not - move to the next granule\n",
    "  mask_geo = (lat2D < POI_lat+geo_deviation)&(lat2D > POI_lat-geo_deviation)\\\n",
    "            &(lon2D < POI_lon+geo_deviation)&(lon2D > POI_lon-geo_deviation)\n",
    "        \n",
    "  nwl_EPIC = len(wl)\n",
    "  \n",
    "  for iwl in range(nwl_EPIC):\n",
    "    mask = mask_geo&(aod3D[:, :, iwl] > 0.)\n",
    "\n",
    "    lat_loc = lat2D[mask]\n",
    "    lon_loc = lon2D[mask]\n",
    "    aod_loc = aod3D[mask, iwl]\n",
    "    n_loc = len(aod_loc)\n",
    "#    if n_loc < 1: continue\n",
    "    if n_loc < 1: prod_loc = -0.999\n",
    "    else:\n",
    "      points = np.empty([0,2])\n",
    "      ff = np.empty(0)\n",
    "\n",
    "      for i in range(n_loc):\n",
    "        points = np.append(points, [[lon_loc[i], lat_loc[i]]], axis=0)\n",
    "        ff = np.append(ff, aod_loc[i])\n",
    "    \n",
    "      try:\n",
    "        [prod_loc] = griddata(points, ff, pp, method='linear', fill_value=-0.999, rescale=False)\n",
    "      except:\n",
    "        try:\n",
    "          prod_loc = np.mean(ff)\n",
    "        except: prod_loc = -0.999\n",
    "#    if prod_loc == -999.: continue\n",
    "    fout.write(' %5.0f %6.3f' %(wl[iwl], prod_loc))\n",
    "    print(wl[iwl], prod_loc)\n",
    "\n",
    "  mask = mask_geo&(uvai2D != fv_uvai)\n",
    "\n",
    "  lat_loc = lat2D[mask]\n",
    "  lon_loc = lon2D[mask]\n",
    "  uvai_loc = uvai2D[mask]\n",
    "  n_loc = len(aod_loc)\n",
    "#  if n_loc < 1: continue\n",
    "  if n_loc < 1: prod_loc = -99.\n",
    "  else:\n",
    "    points = np.empty([0,2])\n",
    "    ff = np.empty(0)\n",
    "\n",
    "    for i in range(n_loc):\n",
    "      points = np.append(points, [[lon_loc[i], lat_loc[i]]], axis=0)\n",
    "      ff = np.append(ff, uvai_loc[i])\n",
    "    \n",
    "    try:\n",
    "      [prod_loc] = griddata(points, ff, pp, method='linear', fill_value=-99., rescale=False)\n",
    "    except:\n",
    "      try:\n",
    "        prod_loc = np.mean(ff)\n",
    "      except: prod_loc = -99.\n",
    "  fout.write(' %6.2f\\n' %prod_loc)\n",
    "  print('UVAI', prod_loc)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c782a-c020-4205-8455-2651c0957302",
   "metadata": {},
   "source": [
    "# 7 Work with TEMPO data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9e1e5-5bce-41fa-8be1-20057f0815c9",
   "metadata": {},
   "source": [
    "## 7.1 Search for TEMPO data files within 0.5 degree range around the POI\n",
    "(position of the AERONET station) falling into the timeframe of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95140f63-b6bf-4813-907b-ba1d5ae60a64",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 6\n",
      "total number of TEMPO version  V02  granules found for POI CCNY \n",
      "within period of interes between 2023-08-02 00:00:00 and 2023-08-02 23:59:59  is 6\n",
      "https://data.asdc.earthdata.nasa.gov/asdc-prod-protected/TEMPO/TEMPO_O3TOT_L2_V02/2023.08.02/TEMPO_O3TOT_L2_V02_20230802T152515Z_S001G03.nc\n",
      "https://data.asdc.earthdata.nasa.gov/asdc-prod-protected/TEMPO/TEMPO_O3TOT_L2_V02/2023.08.02/TEMPO_O3TOT_L2_V02_20230802T162746Z_S002G03.nc\n",
      "https://data.asdc.earthdata.nasa.gov/asdc-prod-protected/TEMPO/TEMPO_O3TOT_L2_V02/2023.08.02/TEMPO_O3TOT_L2_V02_20230802T173017Z_S003G03.nc\n",
      "https://data.asdc.earthdata.nasa.gov/asdc-prod-protected/TEMPO/TEMPO_O3TOT_L2_V02/2023.08.02/TEMPO_O3TOT_L2_V02_20230802T183248Z_S004G03.nc\n",
      "https://data.asdc.earthdata.nasa.gov/asdc-prod-protected/TEMPO/TEMPO_O3TOT_L2_V02/2023.08.02/TEMPO_O3TOT_L2_V02_20230802T193519Z_S005G03.nc\n",
      "https://data.asdc.earthdata.nasa.gov/asdc-prod-protected/TEMPO/TEMPO_O3TOT_L2_V02/2023.08.02/TEMPO_O3TOT_L2_V02_20230802T203750Z_S006G03.nc\n"
     ]
    }
   ],
   "source": [
    "# Setting TEMPO name constants\n",
    "short_name = 'TEMPO_O3TOT_L2' # collection name to search for in the EarthData\n",
    "out_Q = 'UVAI_TEMPO' # name of the output quantity with unit\n",
    "\n",
    "# Searching TEMPO data files within 0.5 degree range around the POI\n",
    "bbox = (POI_lon - 0.5, POI_lat - 0.5, POI_lon + 0.5, POI_lat + 0.5)\n",
    "\n",
    "for version in ['V02', 'V01']:\n",
    "  POI_results = earthaccess.search_data(short_name = short_name\\\n",
    "                                      , version = version\\\n",
    "                                      , temporal = (date_start, date_end)\\\n",
    "                                      , bounding_box = bbox)\n",
    "  n_gr = len(POI_results)\n",
    "  if n_gr == 0: continue\n",
    "  print('total number of TEMPO version ', version,' granules found for POI', POI_name, \\\n",
    "        '\\nwithin period of interes between', date_start, 'and', date_end, ' is', n_gr)\n",
    "  break\n",
    "\n",
    "if n_gr == 0:\n",
    "  print('program terminated')\n",
    "  sys.exit()\n",
    "\n",
    "# Print links to the granules.\n",
    "granule_links = []\n",
    "for result in POI_results: granule_links.append(result['umm']['RelatedUrls'][0]['URL'])\n",
    "for granule_link in granule_links: print(granule_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455feddc-c1fb-4b59-b384-b1cc83efa334",
   "metadata": {},
   "source": [
    "## 7.2 download TEMPO files\n",
    "check whether all downloads were successful,\n",
    "try to download failed granules yet another time.\n",
    "if second attempt fails, remove those granules from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9a1aee3-b30a-49ff-8119-a6241e3b9cb2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Getting 6 granules, approx download size: 0.0 GB\n",
      "Accessing cloud dataset using dataset endpoint credentials: https://data.asdc.earthdata.nasa.gov/s3credentials\n",
      "Downloaded: TEMPO_O3TOT_L2_V02_20230802T152515Z_S001G03.nc\n",
      "Downloaded: TEMPO_O3TOT_L2_V02_20230802T162746Z_S002G03.nc\n",
      "Downloaded: TEMPO_O3TOT_L2_V02_20230802T173017Z_S003G03.nc\n",
      "Downloaded: TEMPO_O3TOT_L2_V02_20230802T183248Z_S004G03.nc\n",
      "Downloaded: TEMPO_O3TOT_L2_V02_20230802T193519Z_S005G03.nc\n",
      "Downloaded: TEMPO_O3TOT_L2_V02_20230802T203750Z_S006G03.nc\n"
     ]
    }
   ],
   "source": [
    "# Downloading TEMPO data files\n",
    "downloaded_files = earthaccess.download(\n",
    "    POI_results,\n",
    "    local_path='.')\n",
    "\n",
    "# Checking whether all TEMPO data files have been downloaded\n",
    "for granule_link in granule_links:\n",
    "  TEMPO_fname = granule_link.split('/')[-1]\n",
    "# check if file exists in the local directory\n",
    "  if not os.path.exists(TEMPO_fname):\n",
    "    print(TEMPO_fname, 'does not exist in local directory')\n",
    "# repeat attempt to download\n",
    "    downloaded_files = earthaccess.download(granule_link,\n",
    "                                            local_path='.')\n",
    "# if file still does not exist in the directory, remove its link from the list of links\n",
    "    if not os.path.exists(TEMPO_fname): granule_links.remove(granule_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7b5d1-89b8-4ff3-a7ac-9a211196e245",
   "metadata": {},
   "source": [
    "## 7.3 compile TEMPO UVAI timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a593984f-b909-4a6c-96fe-f615b4a68278",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "granule  TEMPO_O3TOT_L2_V02_20230802T152515Z_S001G03.nc\n",
      "granule has 123 scanlines by 2048 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117/3270878894.py:15: UserWarning: WARNING: valid_max not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  uvai_QF = np.array(var_QF)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polygon shape:  (4338, 2)\n",
      "point POINT (-73.949036 40.821313) is in granule polygon\n",
      "scanl pixel latitude longitude UVAI UVAI_QF\n",
      "   56  660 40.833450 -73.928078  -0.9      8\n",
      "   56  661 40.812603 -73.934303  -0.9      8\n",
      "   57  660 40.832718 -73.988052  -1.1      0\n",
      "   57  661 40.811600 -73.994415  -0.5      0\n",
      "POI CCNY at 40.821313 N  -73.949036 E found\n",
      "\n",
      "granule  TEMPO_O3TOT_L2_V02_20230802T162746Z_S002G03.nc\n",
      "granule has 123 scanlines by 2048 pixels\n",
      "polygon shape:  (4338, 2)\n",
      "point POINT (-73.949036 40.821313) is in granule polygon\n",
      "scanl pixel latitude longitude UVAI UVAI_QF\n",
      "   56  660 40.837948 -73.902550  -1.2      8\n",
      "   56  661 40.816948 -73.908882  -1.0      8\n",
      "   57  660 40.837009 -73.960854  -1.2      8\n",
      "   57  661 40.815876 -73.967239  -1.0      8\n",
      "POI CCNY at 40.821313 N  -73.949036 E found\n",
      "\n",
      "granule  TEMPO_O3TOT_L2_V02_20230802T173017Z_S003G03.nc\n",
      "granule has 123 scanlines by 2048 pixels\n",
      "polygon shape:  (4338, 2)\n",
      "point POINT (-73.949036 40.821313) is in granule polygon\n",
      "scanl pixel latitude longitude UVAI UVAI_QF\n",
      "   56  661 40.824577 -73.895859  -2.4      8\n",
      "   56  662 40.803699 -73.902115  -1.6      8\n",
      "   57  661 40.822399 -73.953369  -1.9      8\n",
      "   57  662 40.801186 -73.959801  -2.5      8\n",
      "POI CCNY at 40.821313 N  -73.949036 E found\n",
      "\n",
      "granule  TEMPO_O3TOT_L2_V02_20230802T183248Z_S004G03.nc\n",
      "granule has 123 scanlines by 2048 pixels\n",
      "polygon shape:  (4338, 2)\n",
      "point POINT (-73.949036 40.821313) is in granule polygon\n",
      "scanl pixel latitude longitude UVAI UVAI_QF\n",
      "   56  661 40.828724 -73.888702  -1.7      8\n",
      "   56  662 40.807762 -73.895004  -2.1      8\n",
      "   57  661 40.826321 -73.948219  -2.1      8\n",
      "   57  662 40.805107 -73.954651  -2.0      8\n",
      "POI CCNY at 40.821313 N  -73.949036 E found\n",
      "\n",
      "granule  TEMPO_O3TOT_L2_V02_20230802T193519Z_S005G03.nc\n",
      "granule has 123 scanlines by 2048 pixels\n",
      "polygon shape:  (4338, 2)\n",
      "point POINT (-73.949036 40.821313) is in granule polygon\n",
      "scanl pixel latitude longitude UVAI UVAI_QF\n",
      "   57  660 40.834518 -73.925705  -2.3     13\n",
      "   57  661 40.813732 -73.931885  -2.3      8\n",
      "   58  660 40.834290 -73.983162  -1.3      0\n",
      "   58  661 40.813187 -73.989510  -2.3      0\n",
      "POI CCNY at 40.821313 N  -73.949036 E found\n",
      "\n",
      "granule  TEMPO_O3TOT_L2_V02_20230802T203750Z_S006G03.nc\n",
      "granule has 123 scanlines by 2048 pixels\n",
      "polygon shape:  (4338, 2)\n",
      "point POINT (-73.949036 40.821313) is in granule polygon\n",
      "scanl pixel latitude longitude UVAI UVAI_QF\n",
      "   57  661 40.824669 -73.931580  -1.3      8\n",
      "   57  662 40.803707 -73.937866  -2.1      8\n",
      "   58  661 40.821865 -73.990288  -2.0      8\n",
      "   58  662 40.800987 -73.996506  -2.2      8\n",
      "POI CCNY at 40.821313 N  -73.949036 E found\n"
     ]
    }
   ],
   "source": [
    "days = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\n",
    "fout = open(out_Q+'_'+datestamp_ini+'_'+datestamp_fin+'_'\\\n",
    "+POI_name+'_'+str('%08.4fN_%08.4fW.txt' %(POI_lat, -POI_lon)), 'w')\n",
    "fout.write('timeseries of '+out_Q+' at '+POI_name+' '+str('%08.4fN %08.4fW' %(POI_lat, -POI_lon))+'\\n')\n",
    "fout.write('yyyy mm dd hh mn ss time,days   UVAI\\n')\n",
    "\n",
    "pp = np.array([POI_lon, POI_lat])\n",
    "p = Point(pp)\n",
    "\n",
    "for granule_link in sorted(granule_links):\n",
    "  last_slash_ind = granule_link.rfind('/')\n",
    "  fname = granule_link[last_slash_ind+1 : ]\n",
    "  print('\\ngranule ', fname)\n",
    "  lat, lon, fv_geo, time, uvai, uvai_QF, uvai_fv = read_TEMPO_O3TOT_L2_UVAI(fname)\n",
    "\n",
    "  if isinstance(lat, float): continue\n",
    "\n",
    "  polygon = TEMPO_L2_polygon(lat, lon, fv_geo) # create granule polygon\n",
    "\n",
    "  coords_poly = list(polygon)\n",
    "  poly = Polygon(coords_poly)\n",
    "\n",
    "  nx = lon.shape[0]\n",
    "  ny = lon.shape[1]\n",
    "\n",
    "# getting time from the granule filename.\n",
    "# This time corresponds to the 1st element of array time above; that is GPS time in seconds\n",
    "  Tind = fname.rfind('T')\n",
    "  yyyy= int(fname[Tind-8 : Tind-4])\n",
    "  mm = int(fname[Tind-4 : Tind-2])\n",
    "  dd = int(fname[Tind-2 : Tind])\n",
    "  hh = int(fname[Tind+1 : Tind+3])\n",
    "  mn = int(fname[Tind+3 : Tind+5])\n",
    "  ss = float(fname[Tind+5 : Tind+7])\n",
    "\n",
    "# check whether POI is in the granule. If not - move to the next granule\n",
    "  if not p.within(poly): continue\n",
    "  print('point', p, 'is in granule polygon' )\n",
    "\n",
    "  POI_found = False\n",
    "  for ix in range(nx-1):\n",
    "    for iy in range(ny-1):\n",
    "      if lon[ix, iy] == fv_geo: continue\n",
    "      if lat[ix, iy] == fv_geo: continue\n",
    "      if lon[ix, iy+1] == fv_geo: continue\n",
    "      if lat[ix, iy+1] == fv_geo: continue\n",
    "      if lon[ix+1, iy+1] == fv_geo: continue\n",
    "      if lat[ix+1, iy+1] == fv_geo: continue\n",
    "      if lon[ix+1, iy] == fv_geo: continue\n",
    "      if lat[ix+1, iy] == fv_geo: continue\n",
    "\n",
    "      coords_poly_loc = [[lon[ix, iy], lat[ix, iy]], [lon[ix, iy+1], lat[ix, iy+1]] \\\n",
    "                   , [lon[ix+1, iy+1], lat[ix+1, iy+1]], [lon[ix+1, iy], lat[ix+1, iy]]]\n",
    "      poly_loc = Polygon(coords_poly_loc)\n",
    "\n",
    "      if p.within(poly_loc):\n",
    "        print('scanl pixel latitude longitude UVAI UVAI_QF')\n",
    "        for scl in range(ix, ix+2, 1):\n",
    "          for pix in range(iy, iy+2, 1):\n",
    "            print(\"  %3d %4d %9.6f %10.6f %5.1f %6i\"\\\n",
    "  %(scl, pix, lat[scl, pix], lon[scl, pix]\\\n",
    ", uvai[scl, pix], uvai_QF[scl, pix]))\n",
    "\n",
    "        POI_found = True\n",
    "        print('POI', POI_name, 'at', POI_lat,'N ', POI_lon, 'E found')\n",
    "\n",
    "        uvai_loc = np.array([uvai[ix, iy],\\\n",
    "                             uvai[ix, iy+1],\\\n",
    "                             uvai[ix+1, iy+1],\\\n",
    "                             uvai[ix+1, iy]])\n",
    "        uvai_QF_loc = np.array([uvai_QF[ix, iy],\\\n",
    "                                uvai_QF[ix, iy+1],\\\n",
    "                                uvai_QF[ix+1, iy+1],\\\n",
    "                                uvai_QF[ix+1, iy]])\n",
    "        lat_loc = np.array([lat[ix, iy], lat[ix, iy+1],\\\n",
    "                            lat[ix+1, iy+1], lat[ix+1, iy]])\n",
    "        lon_loc = np.array([lon[ix, iy], lon[ix, iy+1],\\\n",
    "                            lon[ix+1, iy+1], lon[ix+1, iy]])\n",
    "#        mask_noFV  = (uvai_loc != uvai_fv)&\\\n",
    "#                     (uvai_QF_loc == 0)\n",
    "        mask_noFV  = (uvai_loc != uvai_fv)\n",
    "\n",
    "        points_noFV  = np.column_stack((lon_loc[mask_noFV], lat_loc[mask_noFV]))\n",
    "        ff_noFV  = uvai_loc[mask_noFV]\n",
    "\n",
    "        points = np.empty([0,2])\n",
    "        ff = np.empty(0)\n",
    "\n",
    "        if ff_noFV.shape[0] == 0:\n",
    "          continue\n",
    "        elif ff_noFV.shape[0] < 4:\n",
    "          uvai_noFV = np.mean(ff_noFV)\n",
    "        elif ff_noFV.shape[0] == 4:\n",
    "          uvai_noFV = griddata(points_noFV, ff_noFV, pp,\\\n",
    "method='linear', fill_value=-99., rescale=False)[0]\n",
    "        if uvai_noFV == -99.: continue\n",
    "\n",
    "# handling time first:\n",
    "        delta_t = (time[ix+1] + time[ix])*0.5 - time[0]\n",
    "        ss = ss + delta_t\n",
    "        if ss >= 60.:\n",
    "          delta_mn = int(ss/60.)\n",
    "          ss = ss - 60.*delta_mn\n",
    "          mn = mn + delta_mn\n",
    "          if mn >= 60:\n",
    "            mn = mn - 60\n",
    "            hh = hh + 1\n",
    "            if hh == 24:\n",
    "              hh = hh - 24\n",
    "              dd = dd + 1\n",
    "              day_month = days[mm]\n",
    "              if (yyyy//4)*4 == yyyy and mm == 2: day_month = day_month + 1\n",
    "              if dd > day_month:\n",
    "                dd = 1\n",
    "                mm = mm + 1\n",
    "                if mm > 12:\n",
    "                  mm = 1\n",
    "                  yyyy = yyyy + 1\n",
    "\n",
    "        int_ss = int(ss)\n",
    "        us = int((ss - int_ss)*10**6)\n",
    "        dt_TEMPO = datetime(yyyy, mm, dd, hh, mn, int_ss, us)\n",
    "        dt_loc = (dt_TEMPO - dt_ini).total_seconds()/86400\n",
    "\n",
    "        fout.write(str('%4.4i %2.2i %2.2i %2.2i %2.2i %2.2i %9.6f %10.3e '\\\n",
    "  %(yyyy, mm, dd, hh, mn, ss, dt_loc, uvai_noFV)))\n",
    "        fout.write(str('%9.4fN %9.4fW %10.3e '\\\n",
    "  %(lat[ix, iy], -lon[ix, iy], uvai[ix, iy])))\n",
    "        fout.write(str('%9.4fN %9.4fW %10.3e '\\\n",
    "  %(lat[ix, iy+1], -lon[ix, iy+1], uvai[ix, iy+1])))\n",
    "        fout.write(str('%9.4fN %9.4fW %10.3e '\\\n",
    "  %(lat[ix+1, iy+1], -lon[ix+1, iy+1], uvai[ix+1, iy+1])))\n",
    "        fout.write(str('%9.4fN %9.4fW %10.3e\\n'\\\n",
    "  %(lat[ix+1, iy], -lon[ix+1, iy], uvai[ix+1, iy])))\n",
    "\n",
    "        break\n",
    "\n",
    "    if POI_found: break\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ae2a9-26fe-43dd-8299-05f97b580386",
   "metadata": {},
   "source": [
    "# 8 reading back timeseries written in the files and plot the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b323ef3-edb4-490e-b352-1ee651c4eadb",
   "metadata": {},
   "source": [
    "## 8.1 AERONET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11d5bf99-51c5-4031-b66e-f61deed751c5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fin = open(out_Q_AERONET+'_'+datestamp_ini+'_'+datestamp_fin+'_'\\\n",
    "+POI_name+'_'+str('%08.4fN_%08.4fW.txt' %(POI_lat, -POI_lon)), 'r')\n",
    "data_lines = fin.readlines()\n",
    "fin.close()\n",
    "\n",
    "timeseries_AERONET_340 = np.empty([0, 2])\n",
    "timeseries_AERONET_380 = np.empty([0, 2])\n",
    "timeseries_AERONET_500 = np.empty([0, 2])\n",
    "for data_line in data_lines[2 : ]:\n",
    "  split = data_line.split()\n",
    "  tt = float(split[6])\n",
    "  aod_340 = float(split[8])\n",
    "  aod_380 = float(split[10])\n",
    "  aod_500 = float(split[12])\n",
    "  if aod_340 > 0.: timeseries_AERONET_340 = np.append(timeseries_AERONET_340, [[tt, aod_340]], axis = 0)\n",
    "  if aod_380 > 0.: timeseries_AERONET_380 = np.append(timeseries_AERONET_380, [[tt, aod_380]], axis = 0)\n",
    "  if aod_500 > 0.: timeseries_AERONET_500 = np.append(timeseries_AERONET_500, [[tt, aod_500]], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9739261-b3e4-4801-8cb8-799f1611f8e3",
   "metadata": {},
   "source": [
    "## 8.2 DSCOVR EPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad788fc3-3bd3-492f-8696-de94619af4e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fin = open(out_Q_EPIC+'_'+datestamp_ini+'_'+datestamp_fin+'_'\\\n",
    "+POI_name+'_'+str('%08.4fN_%08.4fW.txt' %(POI_lat, -POI_lon)), 'r')\n",
    "data_lines = fin.readlines()\n",
    "fin.close()\n",
    "\n",
    "timeseries_EPIC_340 = np.empty([0, 2])\n",
    "timeseries_EPIC_388 = np.empty([0, 2])\n",
    "timeseries_EPIC_500 = np.empty([0, 2])\n",
    "timeseries_EPIC_UVAI = np.empty([0, 2])\n",
    "for data_line in data_lines[2 : ]:\n",
    "  split = data_line.split()\n",
    "  tt = float(split[6])\n",
    "  aod_EPIC_340 = float(split[8])\n",
    "  aod_EPIC_388 = float(split[10])\n",
    "  aod_EPIC_500 = float(split[12])\n",
    "  EPIC_UVAI = float(split[13])\n",
    "  if aod_EPIC_340 > 0.: timeseries_EPIC_340 = np.append(timeseries_EPIC_340, [[tt, aod_EPIC_340]], axis = 0)\n",
    "  if aod_EPIC_388 > 0.: timeseries_EPIC_388 = np.append(timeseries_EPIC_388, [[tt, aod_EPIC_388]], axis = 0)\n",
    "  if aod_EPIC_500 > 0.: timeseries_EPIC_500 = np.append(timeseries_EPIC_500, [[tt, aod_EPIC_500]], axis = 0)\n",
    "  if EPIC_UVAI > -99.: timeseries_EPIC_UVAI = np.append(timeseries_EPIC_UVAI, [[tt, EPIC_UVAI]], axis = 0)# third, TEMPO UVAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfef7ba-f7d7-40c9-b444-97e7e6679ed5",
   "metadata": {},
   "source": [
    "## 8.3 TEMPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "291325c4-d603-4b67-81ac-fe2e1343d601",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fin = open(out_Q+'_'+datestamp_ini+'_'+datestamp_fin+'_'\\\n",
    "+POI_name+'_'+str('%08.4fN_%08.4fW.txt' %(POI_lat, -POI_lon)), 'r')\n",
    "data_lines = fin.readlines()\n",
    "fin.close()\n",
    "\n",
    "timeseries_TEMPO_UVAI = np.empty([0, 2])\n",
    "for data_line in data_lines[2 : ]:\n",
    "  split = data_line.split()\n",
    "  tt = float(split[6])\n",
    "  TEMPO_UVAI = float(split[7])\n",
    "  if TEMPO_UVAI > -99.: timeseries_TEMPO_UVAI = np.append(timeseries_TEMPO_UVAI, [[tt, TEMPO_UVAI]], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ae637-61a6-42e9-8cd0-ad8cf1bcf1f0",
   "metadata": {},
   "source": [
    "## 8.4 plot Aerosol Optical Depth (AOD) from AERONET and DSCOVR EPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "587a0613-8831-4246-af2b-ac7043ac4321",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_title = 'AOD_'+datestamp_ini+'_'+datestamp_fin+'\\n'+POI_name\n",
    "img_name = 'AOD_'+datestamp_ini+'_'+datestamp_fin+'_'+POI_name+'.jpg'\n",
    "\n",
    "plt.plot(timeseries_AERONET_340[:, 0], timeseries_AERONET_340[:, 1],\\\n",
    "         label = \"AOD AERONET at 340nm\", linestyle='None', markersize=2,\\\n",
    "         marker='s', markerfacecolor='r', markeredgecolor='r')\n",
    "plt.plot(timeseries_AERONET_380[:, 0], timeseries_AERONET_380[:, 1],\\\n",
    "         label = \"AOD AERONET at 380nm\", linestyle='None', markersize=2,\\\n",
    "         marker='o', markerfacecolor='r', markeredgecolor='r')\n",
    "plt.plot(timeseries_AERONET_500[:, 0], timeseries_AERONET_500[:, 1],\\\n",
    "         label = \"AOD AERONET at 500nm\", linestyle='None', markersize=2,\\\n",
    "         marker='D', markerfacecolor='r', markeredgecolor='r')\n",
    "\n",
    "plt.plot(timeseries_EPIC_340[:, 0], timeseries_EPIC_340[:, 1],\\\n",
    "         label = \"AOD EPIC at 340nm\", linestyle='None', markersize=2,\\\n",
    "         marker='s', markerfacecolor='c', markeredgecolor='c')\n",
    "plt.plot(timeseries_EPIC_388[:, 0], timeseries_EPIC_388[:, 1],\\\n",
    "         label = \"AOD EPIC at 388nm\", linestyle='None', markersize=2,\\\n",
    "         marker='o', markerfacecolor='c', markeredgecolor='c')\n",
    "plt.plot(timeseries_EPIC_500[:, 0], timeseries_EPIC_500[:, 1],\\\n",
    "         label = \"AOD EPIC at 500nm\", linestyle='None', markersize=2,\\\n",
    "         marker='D', markerfacecolor='c', markeredgecolor='c')\n",
    "\n",
    "# Set the range of x-axis\n",
    "l_lim = 0.\n",
    "u_lim = ((dt_fin - dt_ini).total_seconds() + 1.)/86400.\n",
    "plt.xlim(l_lim, u_lim)\n",
    "\n",
    "# Set the range of y-axis\n",
    "l_lim = 0.\n",
    "u_lim = 2.\n",
    "plt.ylim(l_lim, u_lim)\n",
    "\n",
    "plt.xlabel(r'GMT, day from beginning of '+datestamp_ini, fontsize=12)\n",
    "plt.ylabel(r'AOD', fontsize=12)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.title(plot_title+str(', %08.4fN %08.4fW' %(POI_lat, -POI_lon)))\n",
    "plt.savefig(img_name, format='jpg', dpi=300)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c3f08-d4b4-402b-8194-7402966ec43f",
   "metadata": {},
   "source": [
    "## 8.5 plot UV Aerosol Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ac1b096-aa07-4e05-a490-e80849b15582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_title = 'UVAI_'+datestamp_ini+'_'+datestamp_fin+'\\n'+POI_name\n",
    "img_name = 'UVAI_'+datestamp_ini+'_'+datestamp_fin+'_'+POI_name+'.jpg'\n",
    "\n",
    "plt.plot(timeseries_EPIC_UVAI[:, 0], timeseries_EPIC_UVAI[:, 1],\\\n",
    "         label = \"UVAI EPIC\", linestyle='None', markersize=2,\\\n",
    "         marker='o', markerfacecolor='c', markeredgecolor='c')\n",
    "plt.plot(timeseries_TEMPO_UVAI[:, 0], timeseries_TEMPO_UVAI[:, 1],\\\n",
    "         label = \"UVAI TEMPO\", linestyle='None', markersize=2,\\\n",
    "         marker='o', markerfacecolor='m', markeredgecolor='m')\n",
    "\n",
    "# Set the range of x-axis\n",
    "l_lim = 0.\n",
    "u_lim = ((dt_fin - dt_ini).total_seconds() + 1.)/86400.\n",
    "plt.xlim(l_lim, u_lim)\n",
    "\n",
    "# Set the range of y-axis\n",
    "l_lim = -3.\n",
    "u_lim =  3.\n",
    "plt.ylim(l_lim, u_lim)\n",
    "\n",
    "plt.xlabel(r'GMT, day from beginning of '+datestamp_ini, fontsize=12)\n",
    "plt.ylabel(r'UV Aerosol Index', fontsize=12)\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.title(plot_title+str(', %08.4fN %08.4fW' %(POI_lat, -POI_lon)))\n",
    "plt.savefig(img_name, format='jpg', dpi=300)\n",
    "\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
